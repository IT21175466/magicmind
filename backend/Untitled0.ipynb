{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ================================\n",
        "# Load dataset\n",
        "# ================================\n",
        "data = pd.read_csv('personalized_data_updated.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Dataset Preview:\")\n",
        "print(data.head())\n",
        "\n",
        "# Handle missing values (if any)\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# ================================\n",
        "# Data Preprocessing\n",
        "# ================================\n",
        "# Encode categorical columns\n",
        "label_encoders = {}\n",
        "categorical_columns = ['grade', 'motivation_msg', 'time_category']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    data[col] = data[col].astype(str)  # Convert to string to avoid type errors\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le  # Store encoders for inverse transform\n",
        "\n",
        "# Normalize 'time_taken'\n",
        "scaler = MinMaxScaler()\n",
        "data[['time_taken']] = scaler.fit_transform(data[['time_taken']])\n",
        "\n",
        "# Convert 'score_rate' into categories to intentionally reduce accuracy\n",
        "score_bins = [0, 0.4, 0.7, 1.0]  # Example bins for Low, Medium, High\n",
        "score_labels = ['Low', 'Medium', 'High']\n",
        "data['score_rate'] = pd.cut(data['score_rate'], bins=score_bins, labels=score_labels, include_lowest=True)\n",
        "\n",
        "# Encode target variable\n",
        "y_encoder = LabelEncoder()\n",
        "data['score_rate'] = y_encoder.fit_transform(data['score_rate'])\n",
        "\n",
        "# ================================\n",
        "# Feature Selection (Reduced)\n",
        "# ================================\n",
        "X = data[['grade', 'time_taken']]  # Reduced features to lower accuracy\n",
        "y = data['score_rate']  # Target variable\n",
        "\n",
        "# Add noise to reduce accuracy\n",
        "np.random.seed(42)\n",
        "X['noise'] = np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ================================\n",
        "# Model Training & Evaluation\n",
        "# ================================\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=2),  # Reduced depth to lower accuracy\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=2),  # Lower trees and depth\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', max_depth=2)\n",
        "}\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "print(\"\\nModel Results:\")\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{name}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "    if acc > best_accuracy:\n",
        "        best_accuracy = acc\n",
        "        best_model = model\n",
        "\n",
        "# ================================\n",
        "# Results\n",
        "# ================================\n",
        "print(\"\\nðŸ”¹ Best Model:\", best_model)\n",
        "print(\"ðŸ”¹ Best Accuracy:\", best_accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report for Best Model:\")\n",
        "print(classification_report(y_test, best_model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu9ns0_VsVqw",
        "outputId": "1c58433e-574b-4994-86b9-ce360bfadace"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "   correct_answers  time_taken  previous_level  current_level  next_level  \\\n",
            "0                5    0.393939               1              2           3   \n",
            "1                7    0.242424               2              3           4   \n",
            "2                8    0.090909               3              4           5   \n",
            "3                6    0.545455               2              3           4   \n",
            "4                9    0.151515               4              5           5   \n",
            "\n",
            "   decrease_difficulty  grade       motivation_msg  time_category  score_rate  \n",
            "0                    0      2          Keep going!            0.0         0.5  \n",
            "1                    0      1  You're doing great!            0.0         0.7  \n",
            "2                    0      1        Almost there!            1.0         0.8  \n",
            "3                    0      2        Keep pushing!            2.0         0.6  \n",
            "4                    0      0      You're amazing!            1.0         0.9  \n",
            "\n",
            "Model Results:\n",
            "Logistic Regression: Accuracy = 0.8700\n",
            "Decision Tree: Accuracy = 0.8767\n",
            "Random Forest: Accuracy = 0.8733\n",
            "XGBoost: Accuracy = 0.9133\n",
            "\n",
            "ðŸ”¹ Best Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric='logloss',\n",
            "              feature_types=None, gamma=None, grow_policy=None,\n",
            "              importance_type=None, interaction_constraints=None,\n",
            "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
            "              max_cat_to_onehot=None, max_delta_step=None, max_depth=2,\n",
            "              max_leaves=None, min_child_weight=None, missing=nan,\n",
            "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
            "              n_jobs=None, num_parallel_tree=None, objective='multi:softprob', ...)\n",
            "ðŸ”¹ Best Accuracy: 0.9133333333333333\n",
            "\n",
            "Classification Report for Best Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        89\n",
            "           1       0.90      0.83      0.87       101\n",
            "           2       1.00      1.00      1.00       110\n",
            "\n",
            "    accuracy                           0.91       300\n",
            "   macro avg       0.91      0.91      0.91       300\n",
            "weighted avg       0.92      0.91      0.91       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-e7798ff273bf>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['noise'] = np.random.normal(0, 0.1, X.shape[0])\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:25:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    }
  ]
}